{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsaine/Projet_final_ML/blob/main/projet_final_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "av-ylR1rmGmu",
        "outputId": "38401c83-e8a2-4d95-b99a-899499356904"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open Covid.zip, Covid.zip.zip or Covid.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!unzip Covid.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5HwKGN_NVDY",
        "outputId": "e83e187d-30cb-4b05-9584-739271d87c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn \n",
        "import seaborn as sns\n",
        "#stocker la base de donnée \n",
        "df_path='/Covid Data.csv'\n"
      ],
      "metadata": {
        "id": "RRCgnU8Tq4yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lire la base de donnée\n",
        "df=pd.read_csv(df_path) \n",
        "#pour voir les Paramètres statistiques\n",
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "8oEnEDVnsHI-",
        "outputId": "6025ad27-b0ea-42d8-c4e2-e00592653b0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              USMER  MEDICAL_UNIT           SEX  PATIENT_TYPE       INTUBED  \\\n",
              "count  1.048575e+06  1.048575e+06  1.048575e+06  1.048575e+06  1.048575e+06   \n",
              "mean   1.632194e+00  8.980565e+00  1.499259e+00  1.190765e+00  7.952288e+01   \n",
              "std    4.822084e-01  3.723278e+00  4.999997e-01  3.929041e-01  3.686889e+01   \n",
              "min    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
              "25%    1.000000e+00  4.000000e+00  1.000000e+00  1.000000e+00  9.700000e+01   \n",
              "50%    2.000000e+00  1.200000e+01  1.000000e+00  1.000000e+00  9.700000e+01   \n",
              "75%    2.000000e+00  1.200000e+01  2.000000e+00  1.000000e+00  9.700000e+01   \n",
              "max    2.000000e+00  1.300000e+01  2.000000e+00  2.000000e+00  9.900000e+01   \n",
              "\n",
              "          PNEUMONIA           AGE      PREGNANT      DIABETES          COPD  \\\n",
              "count  1.048575e+06  1.048575e+06  1.048575e+06  1.048575e+06  1.048575e+06   \n",
              "mean   3.346831e+00  4.179410e+01  4.976558e+01  2.186404e+00  2.260569e+00   \n",
              "std    1.191288e+01  1.690739e+01  4.751073e+01  5.424242e+00  5.132258e+00   \n",
              "min    1.000000e+00  0.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
              "25%    2.000000e+00  3.000000e+01  2.000000e+00  2.000000e+00  2.000000e+00   \n",
              "50%    2.000000e+00  4.000000e+01  9.700000e+01  2.000000e+00  2.000000e+00   \n",
              "75%    2.000000e+00  5.300000e+01  9.700000e+01  2.000000e+00  2.000000e+00   \n",
              "max    9.900000e+01  1.210000e+02  9.800000e+01  9.800000e+01  9.800000e+01   \n",
              "\n",
              "             ASTHMA       INMSUPR  HIPERTENSION  OTHER_DISEASE  \\\n",
              "count  1.048575e+06  1.048575e+06  1.048575e+06   1.048575e+06   \n",
              "mean   2.242626e+00  2.298132e+00  2.128989e+00   2.435143e+00   \n",
              "std    5.114089e+00  5.462843e+00  5.236397e+00   6.646676e+00   \n",
              "min    1.000000e+00  1.000000e+00  1.000000e+00   1.000000e+00   \n",
              "25%    2.000000e+00  2.000000e+00  2.000000e+00   2.000000e+00   \n",
              "50%    2.000000e+00  2.000000e+00  2.000000e+00   2.000000e+00   \n",
              "75%    2.000000e+00  2.000000e+00  2.000000e+00   2.000000e+00   \n",
              "max    9.800000e+01  9.800000e+01  9.800000e+01   9.800000e+01   \n",
              "\n",
              "       CARDIOVASCULAR       OBESITY  RENAL_CHRONIC       TOBACCO  \\\n",
              "count    1.048575e+06  1.048575e+06   1.048575e+06  1.048575e+06   \n",
              "mean     2.261810e+00  2.125176e+00   2.257180e+00  2.214333e+00   \n",
              "std      5.194850e+00  5.175445e+00   5.135354e+00  5.323097e+00   \n",
              "min      1.000000e+00  1.000000e+00   1.000000e+00  1.000000e+00   \n",
              "25%      2.000000e+00  2.000000e+00   2.000000e+00  2.000000e+00   \n",
              "50%      2.000000e+00  2.000000e+00   2.000000e+00  2.000000e+00   \n",
              "75%      2.000000e+00  2.000000e+00   2.000000e+00  2.000000e+00   \n",
              "max      9.800000e+01  9.800000e+01   9.800000e+01  9.800000e+01   \n",
              "\n",
              "       CLASIFFICATION_FINAL           ICU  \n",
              "count          1.048575e+06  1.048575e+06  \n",
              "mean           5.305653e+00  7.955397e+01  \n",
              "std            1.881165e+00  3.682307e+01  \n",
              "min            1.000000e+00  1.000000e+00  \n",
              "25%            3.000000e+00  9.700000e+01  \n",
              "50%            6.000000e+00  9.700000e+01  \n",
              "75%            7.000000e+00  9.700000e+01  \n",
              "max            7.000000e+00  9.900000e+01  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3499ba31-b943-4132-8ae1-4431e7e7b3e8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>USMER</th>\n",
              "      <th>MEDICAL_UNIT</th>\n",
              "      <th>SEX</th>\n",
              "      <th>PATIENT_TYPE</th>\n",
              "      <th>INTUBED</th>\n",
              "      <th>PNEUMONIA</th>\n",
              "      <th>AGE</th>\n",
              "      <th>PREGNANT</th>\n",
              "      <th>DIABETES</th>\n",
              "      <th>COPD</th>\n",
              "      <th>ASTHMA</th>\n",
              "      <th>INMSUPR</th>\n",
              "      <th>HIPERTENSION</th>\n",
              "      <th>OTHER_DISEASE</th>\n",
              "      <th>CARDIOVASCULAR</th>\n",
              "      <th>OBESITY</th>\n",
              "      <th>RENAL_CHRONIC</th>\n",
              "      <th>TOBACCO</th>\n",
              "      <th>CLASIFFICATION_FINAL</th>\n",
              "      <th>ICU</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.048575e+06</td>\n",
              "      <td>1.048575e+06</td>\n",
              "      <td>1.048575e+06</td>\n",
              "      <td>1.048575e+06</td>\n",
              "      <td>1.048575e+06</td>\n",
              "      <td>1.048575e+06</td>\n",
              "      <td>1.048575e+06</td>\n",
              "      <td>1.048575e+06</td>\n",
              "      <td>1.048575e+06</td>\n",
              "      <td>1.048575e+06</td>\n",
              "      <td>1.048575e+06</td>\n",
              "      <td>1.048575e+06</td>\n",
              "      <td>1.048575e+06</td>\n",
              "      <td>1.048575e+06</td>\n",
              "      <td>1.048575e+06</td>\n",
              "      <td>1.048575e+06</td>\n",
              "      <td>1.048575e+06</td>\n",
              "      <td>1.048575e+06</td>\n",
              "      <td>1.048575e+06</td>\n",
              "      <td>1.048575e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.632194e+00</td>\n",
              "      <td>8.980565e+00</td>\n",
              "      <td>1.499259e+00</td>\n",
              "      <td>1.190765e+00</td>\n",
              "      <td>7.952288e+01</td>\n",
              "      <td>3.346831e+00</td>\n",
              "      <td>4.179410e+01</td>\n",
              "      <td>4.976558e+01</td>\n",
              "      <td>2.186404e+00</td>\n",
              "      <td>2.260569e+00</td>\n",
              "      <td>2.242626e+00</td>\n",
              "      <td>2.298132e+00</td>\n",
              "      <td>2.128989e+00</td>\n",
              "      <td>2.435143e+00</td>\n",
              "      <td>2.261810e+00</td>\n",
              "      <td>2.125176e+00</td>\n",
              "      <td>2.257180e+00</td>\n",
              "      <td>2.214333e+00</td>\n",
              "      <td>5.305653e+00</td>\n",
              "      <td>7.955397e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.822084e-01</td>\n",
              "      <td>3.723278e+00</td>\n",
              "      <td>4.999997e-01</td>\n",
              "      <td>3.929041e-01</td>\n",
              "      <td>3.686889e+01</td>\n",
              "      <td>1.191288e+01</td>\n",
              "      <td>1.690739e+01</td>\n",
              "      <td>4.751073e+01</td>\n",
              "      <td>5.424242e+00</td>\n",
              "      <td>5.132258e+00</td>\n",
              "      <td>5.114089e+00</td>\n",
              "      <td>5.462843e+00</td>\n",
              "      <td>5.236397e+00</td>\n",
              "      <td>6.646676e+00</td>\n",
              "      <td>5.194850e+00</td>\n",
              "      <td>5.175445e+00</td>\n",
              "      <td>5.135354e+00</td>\n",
              "      <td>5.323097e+00</td>\n",
              "      <td>1.881165e+00</td>\n",
              "      <td>3.682307e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>4.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>9.700000e+01</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>3.000000e+01</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>3.000000e+00</td>\n",
              "      <td>9.700000e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>1.200000e+01</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>9.700000e+01</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>4.000000e+01</td>\n",
              "      <td>9.700000e+01</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>6.000000e+00</td>\n",
              "      <td>9.700000e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>1.200000e+01</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>9.700000e+01</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>5.300000e+01</td>\n",
              "      <td>9.700000e+01</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>7.000000e+00</td>\n",
              "      <td>9.700000e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>1.300000e+01</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>9.900000e+01</td>\n",
              "      <td>9.900000e+01</td>\n",
              "      <td>1.210000e+02</td>\n",
              "      <td>9.800000e+01</td>\n",
              "      <td>9.800000e+01</td>\n",
              "      <td>9.800000e+01</td>\n",
              "      <td>9.800000e+01</td>\n",
              "      <td>9.800000e+01</td>\n",
              "      <td>9.800000e+01</td>\n",
              "      <td>9.800000e+01</td>\n",
              "      <td>9.800000e+01</td>\n",
              "      <td>9.800000e+01</td>\n",
              "      <td>9.800000e+01</td>\n",
              "      <td>9.800000e+01</td>\n",
              "      <td>7.000000e+00</td>\n",
              "      <td>9.900000e+01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3499ba31-b943-4132-8ae1-4431e7e7b3e8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3499ba31-b943-4132-8ae1-4431e7e7b3e8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3499ba31-b943-4132-8ae1-4431e7e7b3e8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "#pour voir les 5 premiers patients\n",
        "df.head()"
      ],
      "metadata": {
        "id": "HoidD1OJsuyh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "b792b972-52f7-4c98-9c95-174fe864147c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   USMER  MEDICAL_UNIT  SEX  PATIENT_TYPE   DATE_DIED  INTUBED  PNEUMONIA  \\\n",
              "0      2             1    1             1  03/05/2020       97          1   \n",
              "1      2             1    2             1  03/06/2020       97          1   \n",
              "2      2             1    2             2  09/06/2020        1          2   \n",
              "3      2             1    1             1  12/06/2020       97          2   \n",
              "4      2             1    2             1  21/06/2020       97          2   \n",
              "\n",
              "   AGE  PREGNANT  DIABETES  ...  ASTHMA  INMSUPR  HIPERTENSION  OTHER_DISEASE  \\\n",
              "0   65         2         2  ...       2        2             1              2   \n",
              "1   72        97         2  ...       2        2             1              2   \n",
              "2   55        97         1  ...       2        2             2              2   \n",
              "3   53         2         2  ...       2        2             2              2   \n",
              "4   68        97         1  ...       2        2             1              2   \n",
              "\n",
              "   CARDIOVASCULAR  OBESITY  RENAL_CHRONIC  TOBACCO  CLASIFFICATION_FINAL  ICU  \n",
              "0               2        2              2        2                     3   97  \n",
              "1               2        1              1        2                     5   97  \n",
              "2               2        2              2        2                     3    2  \n",
              "3               2        2              2        2                     7   97  \n",
              "4               2        2              2        2                     3   97  \n",
              "\n",
              "[5 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4bd9d901-e388-406e-a12f-1607880f2408\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>USMER</th>\n",
              "      <th>MEDICAL_UNIT</th>\n",
              "      <th>SEX</th>\n",
              "      <th>PATIENT_TYPE</th>\n",
              "      <th>DATE_DIED</th>\n",
              "      <th>INTUBED</th>\n",
              "      <th>PNEUMONIA</th>\n",
              "      <th>AGE</th>\n",
              "      <th>PREGNANT</th>\n",
              "      <th>DIABETES</th>\n",
              "      <th>...</th>\n",
              "      <th>ASTHMA</th>\n",
              "      <th>INMSUPR</th>\n",
              "      <th>HIPERTENSION</th>\n",
              "      <th>OTHER_DISEASE</th>\n",
              "      <th>CARDIOVASCULAR</th>\n",
              "      <th>OBESITY</th>\n",
              "      <th>RENAL_CHRONIC</th>\n",
              "      <th>TOBACCO</th>\n",
              "      <th>CLASIFFICATION_FINAL</th>\n",
              "      <th>ICU</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>03/05/2020</td>\n",
              "      <td>97</td>\n",
              "      <td>1</td>\n",
              "      <td>65</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>03/06/2020</td>\n",
              "      <td>97</td>\n",
              "      <td>1</td>\n",
              "      <td>72</td>\n",
              "      <td>97</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>09/06/2020</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>55</td>\n",
              "      <td>97</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12/06/2020</td>\n",
              "      <td>97</td>\n",
              "      <td>2</td>\n",
              "      <td>53</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>21/06/2020</td>\n",
              "      <td>97</td>\n",
              "      <td>2</td>\n",
              "      <td>68</td>\n",
              "      <td>97</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4bd9d901-e388-406e-a12f-1607880f2408')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4bd9d901-e388-406e-a12f-1607880f2408 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4bd9d901-e388-406e-a12f-1607880f2408');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#la taille de base de donnée\n",
        "df.shape"
      ],
      "metadata": {
        "id": "DN6oi9YjPI-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16b134f3-9370-4ccc-9e9d-a32cd3a221ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1048575, 21)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#l’étiquetage \n",
        "df.loc[df['DATE_DIED'] != '9999-99-99'] = '1'\n",
        "df = df.replace('9999-99-99','0')\n"
      ],
      "metadata": {
        "id": "psT0Dwca6gTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decouvrir le type de donnée des varibles de notre base de donnée\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "lRU0b67a0Af3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f887e3b-01f0-42ca-f783-101290cbb8cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "USMER                   object\n",
              "MEDICAL_UNIT            object\n",
              "SEX                     object\n",
              "PATIENT_TYPE            object\n",
              "DATE_DIED               object\n",
              "INTUBED                 object\n",
              "PNEUMONIA               object\n",
              "AGE                     object\n",
              "PREGNANT                object\n",
              "DIABETES                object\n",
              "COPD                    object\n",
              "ASTHMA                  object\n",
              "INMSUPR                 object\n",
              "HIPERTENSION            object\n",
              "OTHER_DISEASE           object\n",
              "CARDIOVASCULAR          object\n",
              "OBESITY                 object\n",
              "RENAL_CHRONIC           object\n",
              "TOBACCO                 object\n",
              "CLASIFFICATION_FINAL    object\n",
              "ICU                     object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#appeler notre base de donnée pour voir si la base est étiqueté ou non \n",
        "df"
      ],
      "metadata": {
        "id": "Bd9sjcKPy-9Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "4a0899db-f877-4383-b184-72fe52e13aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        USMER MEDICAL_UNIT SEX PATIENT_TYPE DATE_DIED INTUBED PNEUMONIA AGE  \\\n",
              "0           1            1   1            1         1       1         1   1   \n",
              "1           1            1   1            1         1       1         1   1   \n",
              "2           1            1   1            1         1       1         1   1   \n",
              "3           1            1   1            1         1       1         1   1   \n",
              "4           1            1   1            1         1       1         1   1   \n",
              "...       ...          ...  ..          ...       ...     ...       ...  ..   \n",
              "1048570     2           13   2            1         0      97         2  40   \n",
              "1048571     1           13   2            2         0       2         2  51   \n",
              "1048572     2           13   2            1         0      97         2  55   \n",
              "1048573     2           13   2            1         0      97         2  28   \n",
              "1048574     2           13   2            1         0      97         2  52   \n",
              "\n",
              "        PREGNANT DIABETES  ... ASTHMA INMSUPR HIPERTENSION OTHER_DISEASE  \\\n",
              "0              1        1  ...      1       1            1             1   \n",
              "1              1        1  ...      1       1            1             1   \n",
              "2              1        1  ...      1       1            1             1   \n",
              "3              1        1  ...      1       1            1             1   \n",
              "4              1        1  ...      1       1            1             1   \n",
              "...          ...      ...  ...    ...     ...          ...           ...   \n",
              "1048570       97        2  ...      2       2            2             2   \n",
              "1048571       97        2  ...      2       2            1             2   \n",
              "1048572       97        2  ...      2       2            2             2   \n",
              "1048573       97        2  ...      2       2            2             2   \n",
              "1048574       97        2  ...      2       2            2             2   \n",
              "\n",
              "        CARDIOVASCULAR OBESITY RENAL_CHRONIC TOBACCO CLASIFFICATION_FINAL ICU  \n",
              "0                    1       1             1       1                    1   1  \n",
              "1                    1       1             1       1                    1   1  \n",
              "2                    1       1             1       1                    1   1  \n",
              "3                    1       1             1       1                    1   1  \n",
              "4                    1       1             1       1                    1   1  \n",
              "...                ...     ...           ...     ...                  ...  ..  \n",
              "1048570              2       2             2       2                    7  97  \n",
              "1048571              2       2             2       2                    7   2  \n",
              "1048572              2       2             2       2                    7  97  \n",
              "1048573              2       2             2       2                    7  97  \n",
              "1048574              2       2             2       2                    7  97  \n",
              "\n",
              "[1048575 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7fefcf86-9444-4fb1-99df-56ab362ceab2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>USMER</th>\n",
              "      <th>MEDICAL_UNIT</th>\n",
              "      <th>SEX</th>\n",
              "      <th>PATIENT_TYPE</th>\n",
              "      <th>DATE_DIED</th>\n",
              "      <th>INTUBED</th>\n",
              "      <th>PNEUMONIA</th>\n",
              "      <th>AGE</th>\n",
              "      <th>PREGNANT</th>\n",
              "      <th>DIABETES</th>\n",
              "      <th>...</th>\n",
              "      <th>ASTHMA</th>\n",
              "      <th>INMSUPR</th>\n",
              "      <th>HIPERTENSION</th>\n",
              "      <th>OTHER_DISEASE</th>\n",
              "      <th>CARDIOVASCULAR</th>\n",
              "      <th>OBESITY</th>\n",
              "      <th>RENAL_CHRONIC</th>\n",
              "      <th>TOBACCO</th>\n",
              "      <th>CLASIFFICATION_FINAL</th>\n",
              "      <th>ICU</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048570</th>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>97</td>\n",
              "      <td>2</td>\n",
              "      <td>40</td>\n",
              "      <td>97</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048571</th>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>51</td>\n",
              "      <td>97</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048572</th>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>97</td>\n",
              "      <td>2</td>\n",
              "      <td>55</td>\n",
              "      <td>97</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048573</th>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>97</td>\n",
              "      <td>2</td>\n",
              "      <td>28</td>\n",
              "      <td>97</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048574</th>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>97</td>\n",
              "      <td>2</td>\n",
              "      <td>52</td>\n",
              "      <td>97</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1048575 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7fefcf86-9444-4fb1-99df-56ab362ceab2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7fefcf86-9444-4fb1-99df-56ab362ceab2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7fefcf86-9444-4fb1-99df-56ab362ceab2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compter les patients morts et vivant\n",
        "df.DATE_DIED.value_counts()"
      ],
      "metadata": {
        "id": "CaSnH-ZU7pyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1179c71a-8e16-4f39-de9e-9c51c4491a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    971633\n",
              "1     76942\n",
              "Name: DATE_DIED, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#l’élimination des lignes ou il y a des valeurs manquantes\n",
        "new_df=df.dropna()\n",
        "#choisir 10000 de patines parmi la taille de base de donnée\n",
        "new_df = new_df.iloc[:1000,:]\n",
        "new_df.shape\n",
        "new_df.DATE_DIED.value_counts()"
      ],
      "metadata": {
        "id": "ThYuT7jj-dfl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf0c90ff-fd55-4c34-b495-1321de4be664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    690\n",
              "0    310\n",
              "Name: DATE_DIED, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "division de la base de donnée:\n",
        "training\n",
        " training \n",
        " validation\n",
        "test\n"
      ],
      "metadata": {
        "id": "0dXzW303_AaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = new_df['DATE_DIED']\n",
        "X = new_df.drop('DATE_DIED',axis=1,inplace=False)"
      ],
      "metadata": {
        "id": "W7BN3C0XAPKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "R3g92IBl_LrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1,shuffle=True)"
      ],
      "metadata": {
        "id": "htdOANYH_STF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "1yz0cB0NBLVb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36b3a28e-ed4e-4cd8-d824-946e55380702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(900, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "knn avec K=100 le nombre du plus proche voisins"
      ],
      "metadata": {
        "id": "N0jw3bGlNwfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "metadata": {
        "id": "7KSxKRmMBXC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=100)\n",
        "knn.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "5p4hyArgB_98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c29ed863-0660-4dfe-a136-aa0b5cca50c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(n_neighbors=100)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy',knn.score(X_test,y_test))"
      ],
      "metadata": {
        "id": "W8mSPOuQEU-B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "702953a2-bcb5-49b2-9711-deb73693e4c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "svm lineaire "
      ],
      "metadata": {
        "id": "fNEyRXrCSqyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.svm import SVC\n",
        "svm = SVC(kernel='linear')\n",
        "# Train the classifier using the training data\n",
        "svm.fit(X_train,  y_train)\n",
        "# Test the classifier using the test data\n",
        "accuracy2 = svm.score(X_test, y_test)\n",
        "print('Accuracy:', accuracy2)"
      ],
      "metadata": {
        "id": "84XzywUMCUra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb7c52a8-2f0a-4850-d5da-9815f3150162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "svm gaussien"
      ],
      "metadata": {
        "id": "AnnXR6KJS3L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "svmGaussian = SVC(kernel='rbf')\n",
        "# Train the classifier using the training data\n",
        "svmGaussian .fit(X_train,  y_train)\n",
        "# Test the classifier using the test data\n",
        "accuracy2 = svmGaussian.score(X_test, y_test)\n",
        "print('Accuracy:', accuracy2)"
      ],
      "metadata": {
        "id": "Y6_IGs7DIldv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc509c9-a16a-4dc9-b0fa-4448914d8765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "percpton"
      ],
      "metadata": {
        "id": "lkwWl5CJS8PT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp_clf = MLPClassifier(hidden_layer_sizes=(150,100,1),\n",
        "                        max_iter = 100,activation = 'logistic',\n",
        "                        solver = 'adam', verbose = 1)\n",
        "\n",
        "mlp_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "KqhTS6ADJKds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34776ab5-c17b-4cf2-e694-6497a643186f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.70050614\n",
            "Iteration 2, loss = 0.67524440\n",
            "Iteration 3, loss = 0.66023597\n",
            "Iteration 4, loss = 0.65221178\n",
            "Iteration 5, loss = 0.64687512\n",
            "Iteration 6, loss = 0.64150119\n",
            "Iteration 7, loss = 0.63437497\n",
            "Iteration 8, loss = 0.62408744\n",
            "Iteration 9, loss = 0.61113401\n",
            "Iteration 10, loss = 0.59887662\n",
            "Iteration 11, loss = 0.58842611\n",
            "Iteration 12, loss = 0.57962495\n",
            "Iteration 13, loss = 0.57204816\n",
            "Iteration 14, loss = 0.56567220\n",
            "Iteration 15, loss = 0.56044956\n",
            "Iteration 16, loss = 0.55608161\n",
            "Iteration 17, loss = 0.55228511\n",
            "Iteration 18, loss = 0.54886803\n",
            "Iteration 19, loss = 0.54576321\n",
            "Iteration 20, loss = 0.54285877\n",
            "Iteration 21, loss = 0.54015786\n",
            "Iteration 22, loss = 0.53755226\n",
            "Iteration 23, loss = 0.53499285\n",
            "Iteration 24, loss = 0.53260680\n",
            "Iteration 25, loss = 0.53026202\n",
            "Iteration 26, loss = 0.52803779\n",
            "Iteration 27, loss = 0.52582290\n",
            "Iteration 28, loss = 0.52367521\n",
            "Iteration 29, loss = 0.52170796\n",
            "Iteration 30, loss = 0.51969302\n",
            "Iteration 31, loss = 0.51772662\n",
            "Iteration 32, loss = 0.51581559\n",
            "Iteration 33, loss = 0.51387235\n",
            "Iteration 34, loss = 0.51197434\n",
            "Iteration 35, loss = 0.51009533\n",
            "Iteration 36, loss = 0.50825912\n",
            "Iteration 37, loss = 0.50628075\n",
            "Iteration 38, loss = 0.50433384\n",
            "Iteration 39, loss = 0.50235413\n",
            "Iteration 40, loss = 0.50040202\n",
            "Iteration 41, loss = 0.49849966\n",
            "Iteration 42, loss = 0.49656293\n",
            "Iteration 43, loss = 0.49468918\n",
            "Iteration 44, loss = 0.49267752\n",
            "Iteration 45, loss = 0.49082438\n",
            "Iteration 46, loss = 0.48890024\n",
            "Iteration 47, loss = 0.48690781\n",
            "Iteration 48, loss = 0.48506330\n",
            "Iteration 49, loss = 0.48310941\n",
            "Iteration 50, loss = 0.48126638\n",
            "Iteration 51, loss = 0.47935771\n",
            "Iteration 52, loss = 0.47762877\n",
            "Iteration 53, loss = 0.47588787\n",
            "Iteration 54, loss = 0.47428270\n",
            "Iteration 55, loss = 0.47273952\n",
            "Iteration 56, loss = 0.47123823\n",
            "Iteration 57, loss = 0.46974442\n",
            "Iteration 58, loss = 0.46833698\n",
            "Iteration 59, loss = 0.46698117\n",
            "Iteration 60, loss = 0.46562583\n",
            "Iteration 61, loss = 0.46428908\n",
            "Iteration 62, loss = 0.46300121\n",
            "Iteration 63, loss = 0.46173450\n",
            "Iteration 64, loss = 0.46047176\n",
            "Iteration 65, loss = 0.45924598\n",
            "Iteration 66, loss = 0.45803217\n",
            "Iteration 67, loss = 0.45684093\n",
            "Iteration 68, loss = 0.45565107\n",
            "Iteration 69, loss = 0.45451037\n",
            "Iteration 70, loss = 0.45338288\n",
            "Iteration 71, loss = 0.45223886\n",
            "Iteration 72, loss = 0.45113745\n",
            "Iteration 73, loss = 0.45003528\n",
            "Iteration 74, loss = 0.44896317\n",
            "Iteration 75, loss = 0.44789022\n",
            "Iteration 76, loss = 0.44682962\n",
            "Iteration 77, loss = 0.44581060\n",
            "Iteration 78, loss = 0.44476656\n",
            "Iteration 79, loss = 0.44374978\n",
            "Iteration 80, loss = 0.44276502\n",
            "Iteration 81, loss = 0.44177021\n",
            "Iteration 82, loss = 0.44078516\n",
            "Iteration 83, loss = 0.43980593\n",
            "Iteration 84, loss = 0.43886256\n",
            "Iteration 85, loss = 0.43790521\n",
            "Iteration 86, loss = 0.43698830\n",
            "Iteration 87, loss = 0.43601659\n",
            "Iteration 88, loss = 0.43511179\n",
            "Iteration 89, loss = 0.43419680\n",
            "Iteration 90, loss = 0.43332138\n",
            "Iteration 91, loss = 0.43241508\n",
            "Iteration 92, loss = 0.43152506\n",
            "Iteration 93, loss = 0.43066534\n",
            "Iteration 94, loss = 0.42980123\n",
            "Iteration 95, loss = 0.42894008\n",
            "Iteration 96, loss = 0.42808712\n",
            "Iteration 97, loss = 0.42726709\n",
            "Iteration 98, loss = 0.42642950\n",
            "Iteration 99, loss = 0.42560788\n",
            "Iteration 100, loss = 0.42480317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='logistic', hidden_layer_sizes=(150, 100, 1),\n",
              "              max_iter=100, verbose=1)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = mlp_clf.predict(X_test)\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "Rj69ihMJKrNz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fdff4ab-d18b-4944-ccce-f7487a1b7206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        37\n",
            "           1       0.63      1.00      0.77        63\n",
            "\n",
            "    accuracy                           0.63       100\n",
            "   macro avg       0.32      0.50      0.39       100\n",
            "weighted avg       0.40      0.63      0.49       100\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "la validation croisee des notres modeles avec k-fold=10"
      ],
      "metadata": {
        "id": "LvQTkpQ5jMm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "accuracy1 = cross_val_score(knn,X_train,y_train,scoring = \"accuracy\",cv = 10)\n",
        "accuracy2 = cross_val_score(svm,X_train,y_train,scoring = \"accuracy\",cv = 10)\n",
        "accuracy3 = cross_val_score(svmGaussian,X_train,y_train,scoring = \"accuracy\",cv = 10)\n",
        "accuracy4 = cross_val_score(mlp_clf,X_train,y_train,scoring = \"accuracy\",cv = 10)"
      ],
      "metadata": {
        "id": "1s3PmxvOO8qh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ad49a3c-ea02-4526-f9a6-aca0d110789d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.61048883\n",
            "Iteration 2, loss = 0.60313273\n",
            "Iteration 3, loss = 0.59559165\n",
            "Iteration 4, loss = 0.58776074\n",
            "Iteration 5, loss = 0.58002895\n",
            "Iteration 6, loss = 0.57285386\n",
            "Iteration 7, loss = 0.56708888\n",
            "Iteration 8, loss = 0.56253488\n",
            "Iteration 9, loss = 0.55879009\n",
            "Iteration 10, loss = 0.55551400\n",
            "Iteration 11, loss = 0.55285062\n",
            "Iteration 12, loss = 0.55049394\n",
            "Iteration 13, loss = 0.54856475\n",
            "Iteration 14, loss = 0.54677111\n",
            "Iteration 15, loss = 0.54508897\n",
            "Iteration 16, loss = 0.54354277\n",
            "Iteration 17, loss = 0.54206045\n",
            "Iteration 18, loss = 0.54073340\n",
            "Iteration 19, loss = 0.53946176\n",
            "Iteration 20, loss = 0.53823814\n",
            "Iteration 21, loss = 0.53705849\n",
            "Iteration 22, loss = 0.53591187\n",
            "Iteration 23, loss = 0.53476724\n",
            "Iteration 24, loss = 0.53361651\n",
            "Iteration 25, loss = 0.53252828\n",
            "Iteration 26, loss = 0.53147143\n",
            "Iteration 27, loss = 0.53040460\n",
            "Iteration 28, loss = 0.52929506\n",
            "Iteration 29, loss = 0.52811736\n",
            "Iteration 30, loss = 0.52696606\n",
            "Iteration 31, loss = 0.52576281\n",
            "Iteration 32, loss = 0.52459466\n",
            "Iteration 33, loss = 0.52340468\n",
            "Iteration 34, loss = 0.52218648\n",
            "Iteration 35, loss = 0.52100279\n",
            "Iteration 36, loss = 0.51983429\n",
            "Iteration 37, loss = 0.51869214\n",
            "Iteration 38, loss = 0.51756459\n",
            "Iteration 39, loss = 0.51645339\n",
            "Iteration 40, loss = 0.51533441\n",
            "Iteration 41, loss = 0.51414624\n",
            "Iteration 42, loss = 0.51287727\n",
            "Iteration 43, loss = 0.51170864\n",
            "Iteration 44, loss = 0.51043315\n",
            "Iteration 45, loss = 0.50924601\n",
            "Iteration 46, loss = 0.50803902\n",
            "Iteration 47, loss = 0.50671687\n",
            "Iteration 48, loss = 0.50545605\n",
            "Iteration 49, loss = 0.50419930\n",
            "Iteration 50, loss = 0.50295651\n",
            "Iteration 51, loss = 0.50178760\n",
            "Iteration 52, loss = 0.50068412\n",
            "Iteration 53, loss = 0.49952900\n",
            "Iteration 54, loss = 0.49846596\n",
            "Iteration 55, loss = 0.49744004\n",
            "Iteration 56, loss = 0.49646276\n",
            "Iteration 57, loss = 0.49549465\n",
            "Iteration 58, loss = 0.49449245\n",
            "Iteration 59, loss = 0.49350211\n",
            "Iteration 60, loss = 0.49248018\n",
            "Iteration 61, loss = 0.49148911\n",
            "Iteration 62, loss = 0.49045963\n",
            "Iteration 63, loss = 0.48946917\n",
            "Iteration 64, loss = 0.48851460\n",
            "Iteration 65, loss = 0.48756459\n",
            "Iteration 66, loss = 0.48661004\n",
            "Iteration 67, loss = 0.48563080\n",
            "Iteration 68, loss = 0.48470523\n",
            "Iteration 69, loss = 0.48379025\n",
            "Iteration 70, loss = 0.48288684\n",
            "Iteration 71, loss = 0.48199568\n",
            "Iteration 72, loss = 0.48108918\n",
            "Iteration 73, loss = 0.48017170\n",
            "Iteration 74, loss = 0.47926127\n",
            "Iteration 75, loss = 0.47835686\n",
            "Iteration 76, loss = 0.47746533\n",
            "Iteration 77, loss = 0.47658872\n",
            "Iteration 78, loss = 0.47571201\n",
            "Iteration 79, loss = 0.47482857\n",
            "Iteration 80, loss = 0.47393559\n",
            "Iteration 81, loss = 0.47305559\n",
            "Iteration 82, loss = 0.47216006\n",
            "Iteration 83, loss = 0.47123934\n",
            "Iteration 84, loss = 0.47034387\n",
            "Iteration 85, loss = 0.46944292\n",
            "Iteration 86, loss = 0.46855407\n",
            "Iteration 87, loss = 0.46762722\n",
            "Iteration 88, loss = 0.46675955\n",
            "Iteration 89, loss = 0.46588210\n",
            "Iteration 90, loss = 0.46502857\n",
            "Iteration 91, loss = 0.46414619\n",
            "Iteration 92, loss = 0.46324487\n",
            "Iteration 93, loss = 0.46241347\n",
            "Iteration 94, loss = 0.46158492\n",
            "Iteration 95, loss = 0.46077054\n",
            "Iteration 96, loss = 0.45995689\n",
            "Iteration 97, loss = 0.45911811\n",
            "Iteration 98, loss = 0.45826061\n",
            "Iteration 99, loss = 0.45742811\n",
            "Iteration 100, loss = 0.45658321\n",
            "Iteration 1, loss = 0.61311667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss = 0.60301337\n",
            "Iteration 3, loss = 0.59396786\n",
            "Iteration 4, loss = 0.58541776\n",
            "Iteration 5, loss = 0.57617972\n",
            "Iteration 6, loss = 0.56679356\n",
            "Iteration 7, loss = 0.55834992\n",
            "Iteration 8, loss = 0.55151332\n",
            "Iteration 9, loss = 0.54598701\n",
            "Iteration 10, loss = 0.54158230\n",
            "Iteration 11, loss = 0.53821893\n",
            "Iteration 12, loss = 0.53547139\n",
            "Iteration 13, loss = 0.53330445\n",
            "Iteration 14, loss = 0.53130985\n",
            "Iteration 15, loss = 0.52955793\n",
            "Iteration 16, loss = 0.52793603\n",
            "Iteration 17, loss = 0.52640128\n",
            "Iteration 18, loss = 0.52490453\n",
            "Iteration 19, loss = 0.52356754\n",
            "Iteration 20, loss = 0.52226881\n",
            "Iteration 21, loss = 0.52096805\n",
            "Iteration 22, loss = 0.51975066\n",
            "Iteration 23, loss = 0.51847420\n",
            "Iteration 24, loss = 0.51722890\n",
            "Iteration 25, loss = 0.51601592\n",
            "Iteration 26, loss = 0.51480374\n",
            "Iteration 27, loss = 0.51362736\n",
            "Iteration 28, loss = 0.51247921\n",
            "Iteration 29, loss = 0.51137093\n",
            "Iteration 30, loss = 0.51026730\n",
            "Iteration 31, loss = 0.50918788\n",
            "Iteration 32, loss = 0.50812586\n",
            "Iteration 33, loss = 0.50705688\n",
            "Iteration 34, loss = 0.50598114\n",
            "Iteration 35, loss = 0.50487805\n",
            "Iteration 36, loss = 0.50367129\n",
            "Iteration 37, loss = 0.50251279\n",
            "Iteration 38, loss = 0.50127858\n",
            "Iteration 39, loss = 0.50005063\n",
            "Iteration 40, loss = 0.49885222\n",
            "Iteration 41, loss = 0.49769853\n",
            "Iteration 42, loss = 0.49648222\n",
            "Iteration 43, loss = 0.49510712\n",
            "Iteration 44, loss = 0.49387184\n",
            "Iteration 45, loss = 0.49279240\n",
            "Iteration 46, loss = 0.49131909\n",
            "Iteration 47, loss = 0.48992201\n",
            "Iteration 48, loss = 0.48875435\n",
            "Iteration 49, loss = 0.48742623\n",
            "Iteration 50, loss = 0.48614658\n",
            "Iteration 51, loss = 0.48503874\n",
            "Iteration 52, loss = 0.48378084\n",
            "Iteration 53, loss = 0.48258298\n",
            "Iteration 54, loss = 0.48147711\n",
            "Iteration 55, loss = 0.48039541\n",
            "Iteration 56, loss = 0.47931885\n",
            "Iteration 57, loss = 0.47822969\n",
            "Iteration 58, loss = 0.47719432\n",
            "Iteration 59, loss = 0.47616983\n",
            "Iteration 60, loss = 0.47521286\n",
            "Iteration 61, loss = 0.47439749\n",
            "Iteration 62, loss = 0.47332893\n",
            "Iteration 63, loss = 0.47215802\n",
            "Iteration 64, loss = 0.47109089\n",
            "Iteration 65, loss = 0.47016148\n",
            "Iteration 66, loss = 0.46928265\n",
            "Iteration 67, loss = 0.46837594\n",
            "Iteration 68, loss = 0.46743537\n",
            "Iteration 69, loss = 0.46656396\n",
            "Iteration 70, loss = 0.46566977\n",
            "Iteration 71, loss = 0.46474034\n",
            "Iteration 72, loss = 0.46379457\n",
            "Iteration 73, loss = 0.46285551\n",
            "Iteration 74, loss = 0.46194227\n",
            "Iteration 75, loss = 0.46105642\n",
            "Iteration 76, loss = 0.46017620\n",
            "Iteration 77, loss = 0.45931691\n",
            "Iteration 78, loss = 0.45845370\n",
            "Iteration 79, loss = 0.45758121\n",
            "Iteration 80, loss = 0.45670624\n",
            "Iteration 81, loss = 0.45582514\n",
            "Iteration 82, loss = 0.45493893\n",
            "Iteration 83, loss = 0.45409101\n",
            "Iteration 84, loss = 0.45325945\n",
            "Iteration 85, loss = 0.45240670\n",
            "Iteration 86, loss = 0.45153382\n",
            "Iteration 87, loss = 0.45064836\n",
            "Iteration 88, loss = 0.44978864\n",
            "Iteration 89, loss = 0.44894111\n",
            "Iteration 90, loss = 0.44810617\n",
            "Iteration 91, loss = 0.44725300\n",
            "Iteration 92, loss = 0.44641051\n",
            "Iteration 93, loss = 0.44558094\n",
            "Iteration 94, loss = 0.44477359\n",
            "Iteration 95, loss = 0.44396193\n",
            "Iteration 96, loss = 0.44314231\n",
            "Iteration 97, loss = 0.44232018\n",
            "Iteration 98, loss = 0.44149270\n",
            "Iteration 99, loss = 0.44067832\n",
            "Iteration 100, loss = 0.43985812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.61248944\n",
            "Iteration 2, loss = 0.60976846\n",
            "Iteration 3, loss = 0.60720167\n",
            "Iteration 4, loss = 0.60408288\n",
            "Iteration 5, loss = 0.60056598\n",
            "Iteration 6, loss = 0.59688404\n",
            "Iteration 7, loss = 0.59329913\n",
            "Iteration 8, loss = 0.59007056\n",
            "Iteration 9, loss = 0.58727363\n",
            "Iteration 10, loss = 0.58492341\n",
            "Iteration 11, loss = 0.58283619\n",
            "Iteration 12, loss = 0.58103174\n",
            "Iteration 13, loss = 0.57935799\n",
            "Iteration 14, loss = 0.57785466\n",
            "Iteration 15, loss = 0.57640321\n",
            "Iteration 16, loss = 0.57498459\n",
            "Iteration 17, loss = 0.57362936\n",
            "Iteration 18, loss = 0.57233015\n",
            "Iteration 19, loss = 0.57101162\n",
            "Iteration 20, loss = 0.56965735\n",
            "Iteration 21, loss = 0.56845360\n",
            "Iteration 22, loss = 0.56727904\n",
            "Iteration 23, loss = 0.56624438\n",
            "Iteration 24, loss = 0.56524749\n",
            "Iteration 25, loss = 0.56422502\n",
            "Iteration 26, loss = 0.56316051\n",
            "Iteration 27, loss = 0.56211503\n",
            "Iteration 28, loss = 0.56102539\n",
            "Iteration 29, loss = 0.55995760\n",
            "Iteration 30, loss = 0.55896220\n",
            "Iteration 31, loss = 0.55789686\n",
            "Iteration 32, loss = 0.55685736\n",
            "Iteration 33, loss = 0.55577940\n",
            "Iteration 34, loss = 0.55469852\n",
            "Iteration 35, loss = 0.55357038\n",
            "Iteration 36, loss = 0.55245685\n",
            "Iteration 37, loss = 0.55130687\n",
            "Iteration 38, loss = 0.55024202\n",
            "Iteration 39, loss = 0.54918168\n",
            "Iteration 40, loss = 0.54814480\n",
            "Iteration 41, loss = 0.54701567\n",
            "Iteration 42, loss = 0.54583856\n",
            "Iteration 43, loss = 0.54467882\n",
            "Iteration 44, loss = 0.54345822\n",
            "Iteration 45, loss = 0.54215235\n",
            "Iteration 46, loss = 0.54084418\n",
            "Iteration 47, loss = 0.53951476\n",
            "Iteration 48, loss = 0.53823150\n",
            "Iteration 49, loss = 0.53710787\n",
            "Iteration 50, loss = 0.53595960\n",
            "Iteration 51, loss = 0.53434635\n",
            "Iteration 52, loss = 0.53322611\n",
            "Iteration 53, loss = 0.53221219\n",
            "Iteration 54, loss = 0.53107255\n",
            "Iteration 55, loss = 0.53003011\n",
            "Iteration 56, loss = 0.52894238\n",
            "Iteration 57, loss = 0.52796248\n",
            "Iteration 58, loss = 0.52691931\n",
            "Iteration 59, loss = 0.52593457\n",
            "Iteration 60, loss = 0.52494644\n",
            "Iteration 61, loss = 0.52397473\n",
            "Iteration 62, loss = 0.52298128\n",
            "Iteration 63, loss = 0.52200047\n",
            "Iteration 64, loss = 0.52103027\n",
            "Iteration 65, loss = 0.52002918\n",
            "Iteration 66, loss = 0.51908401\n",
            "Iteration 67, loss = 0.51813820\n",
            "Iteration 68, loss = 0.51720691\n",
            "Iteration 69, loss = 0.51625746\n",
            "Iteration 70, loss = 0.51531400\n",
            "Iteration 71, loss = 0.51437043\n",
            "Iteration 72, loss = 0.51343719\n",
            "Iteration 73, loss = 0.51249971\n",
            "Iteration 74, loss = 0.51157917\n",
            "Iteration 75, loss = 0.51067177\n",
            "Iteration 76, loss = 0.50971807\n",
            "Iteration 77, loss = 0.50878422\n",
            "Iteration 78, loss = 0.50780917\n",
            "Iteration 79, loss = 0.50686683\n",
            "Iteration 80, loss = 0.50593780\n",
            "Iteration 81, loss = 0.50497176\n",
            "Iteration 82, loss = 0.50406274\n",
            "Iteration 83, loss = 0.50311638\n",
            "Iteration 84, loss = 0.50219348\n",
            "Iteration 85, loss = 0.50125459\n",
            "Iteration 86, loss = 0.50036004\n",
            "Iteration 87, loss = 0.49947834\n",
            "Iteration 88, loss = 0.49859800\n",
            "Iteration 89, loss = 0.49769584\n",
            "Iteration 90, loss = 0.49682647\n",
            "Iteration 91, loss = 0.49597366\n",
            "Iteration 92, loss = 0.49510256\n",
            "Iteration 93, loss = 0.49424308\n",
            "Iteration 94, loss = 0.49336329\n",
            "Iteration 95, loss = 0.49248974\n",
            "Iteration 96, loss = 0.49160103\n",
            "Iteration 97, loss = 0.49072858\n",
            "Iteration 98, loss = 0.48987654\n",
            "Iteration 99, loss = 0.48902268\n",
            "Iteration 100, loss = 0.48817480\n",
            "Iteration 1, loss = 0.76752196"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 2, loss = 0.74806593\n",
            "Iteration 3, loss = 0.73857730\n",
            "Iteration 4, loss = 0.73400208\n",
            "Iteration 5, loss = 0.73116435\n",
            "Iteration 6, loss = 0.72894303\n",
            "Iteration 7, loss = 0.72706450\n",
            "Iteration 8, loss = 0.72519151\n",
            "Iteration 9, loss = 0.72319116\n",
            "Iteration 10, loss = 0.72084885\n",
            "Iteration 11, loss = 0.71784423\n",
            "Iteration 12, loss = 0.71324293\n",
            "Iteration 13, loss = 0.70604613\n",
            "Iteration 14, loss = 0.69609933\n",
            "Iteration 15, loss = 0.68618728\n",
            "Iteration 16, loss = 0.67770624\n",
            "Iteration 17, loss = 0.67050109\n",
            "Iteration 18, loss = 0.66460253\n",
            "Iteration 19, loss = 0.65971796\n",
            "Iteration 20, loss = 0.65577864\n",
            "Iteration 21, loss = 0.65236330\n",
            "Iteration 22, loss = 0.64933843\n",
            "Iteration 23, loss = 0.64651759\n",
            "Iteration 24, loss = 0.64388974\n",
            "Iteration 25, loss = 0.64143186\n",
            "Iteration 26, loss = 0.63901689\n",
            "Iteration 27, loss = 0.63678394\n",
            "Iteration 28, loss = 0.63463079\n",
            "Iteration 29, loss = 0.63254073\n",
            "Iteration 30, loss = 0.63055932\n",
            "Iteration 31, loss = 0.62858830\n",
            "Iteration 32, loss = 0.62661191\n",
            "Iteration 33, loss = 0.62463715\n",
            "Iteration 34, loss = 0.62266170\n",
            "Iteration 35, loss = 0.62072715\n",
            "Iteration 36, loss = 0.61885755\n",
            "Iteration 37, loss = 0.61699997\n",
            "Iteration 38, loss = 0.61513159\n",
            "Iteration 39, loss = 0.61328022\n",
            "Iteration 40, loss = 0.61144047\n",
            "Iteration 41, loss = 0.60957016\n",
            "Iteration 42, loss = 0.60781044\n",
            "Iteration 43, loss = 0.60604840\n",
            "Iteration 44, loss = 0.60429764\n",
            "Iteration 45, loss = 0.60257021\n",
            "Iteration 46, loss = 0.60080654\n",
            "Iteration 47, loss = 0.59908324\n",
            "Iteration 48, loss = 0.59736686\n",
            "Iteration 49, loss = 0.59569640\n",
            "Iteration 50, loss = 0.59408319\n",
            "Iteration 51, loss = 0.59248743\n",
            "Iteration 52, loss = 0.59089484\n",
            "Iteration 53, loss = 0.58930279\n",
            "Iteration 54, loss = 0.58772405\n",
            "Iteration 55, loss = 0.58612517\n",
            "Iteration 56, loss = 0.58452007\n",
            "Iteration 57, loss = 0.58289989\n",
            "Iteration 58, loss = 0.58131694\n",
            "Iteration 59, loss = 0.57971128\n",
            "Iteration 60, loss = 0.57811217\n",
            "Iteration 61, loss = 0.57652221\n",
            "Iteration 62, loss = 0.57493957\n",
            "Iteration 63, loss = 0.57341582\n",
            "Iteration 64, loss = 0.57190032\n",
            "Iteration 65, loss = 0.57037545\n",
            "Iteration 66, loss = 0.56885019\n",
            "Iteration 67, loss = 0.56734184\n",
            "Iteration 68, loss = 0.56584132\n",
            "Iteration 69, loss = 0.56433909\n",
            "Iteration 70, loss = 0.56285104\n",
            "Iteration 71, loss = 0.56138640\n",
            "Iteration 72, loss = 0.55988733\n",
            "Iteration 73, loss = 0.55842432\n",
            "Iteration 74, loss = 0.55695223\n",
            "Iteration 75, loss = 0.55551650\n",
            "Iteration 76, loss = 0.55405376\n",
            "Iteration 77, loss = 0.55262549\n",
            "Iteration 78, loss = 0.55121247\n",
            "Iteration 79, loss = 0.54980448\n",
            "Iteration 80, loss = 0.54836280\n",
            "Iteration 81, loss = 0.54697873\n",
            "Iteration 82, loss = 0.54559046\n",
            "Iteration 83, loss = 0.54415950\n",
            "Iteration 84, loss = 0.54273992\n",
            "Iteration 85, loss = 0.54131362\n",
            "Iteration 86, loss = 0.53993205\n",
            "Iteration 87, loss = 0.53855316\n",
            "Iteration 88, loss = 0.53718832\n",
            "Iteration 89, loss = 0.53581862\n",
            "Iteration 90, loss = 0.53444723\n",
            "Iteration 91, loss = 0.53304922\n",
            "Iteration 92, loss = 0.53166441\n",
            "Iteration 93, loss = 0.53028185\n",
            "Iteration 94, loss = 0.52884505\n",
            "Iteration 95, loss = 0.52736628\n",
            "Iteration 96, loss = 0.52562017\n",
            "Iteration 97, loss = 0.52339406\n",
            "Iteration 98, loss = 0.52113725\n",
            "Iteration 99, loss = 0.51890166\n",
            "Iteration 100, loss = 0.51671998\n",
            "Iteration 1, loss = 0.66871938\n",
            "Iteration 2, loss = 0.65591372\n",
            "Iteration 3, loss = 0.64815117\n",
            "Iteration 4, loss = 0.64384619\n",
            "Iteration 5, loss = 0.63999788\n",
            "Iteration 6, loss = 0.63530540\n",
            "Iteration 7, loss = 0.62860388\n",
            "Iteration 8, loss = 0.61831266\n",
            "Iteration 9, loss = 0.60598020\n",
            "Iteration 10, loss = 0.59494341\n",
            "Iteration 11, loss = 0.58610096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 12, loss = 0.57848691\n",
            "Iteration 13, loss = 0.57221319\n",
            "Iteration 14, loss = 0.56717254\n",
            "Iteration 15, loss = 0.56302396\n",
            "Iteration 16, loss = 0.55952585\n",
            "Iteration 17, loss = 0.55642230\n",
            "Iteration 18, loss = 0.55367014\n",
            "Iteration 19, loss = 0.55110234\n",
            "Iteration 20, loss = 0.54869681\n",
            "Iteration 21, loss = 0.54643973\n",
            "Iteration 22, loss = 0.54424563\n",
            "Iteration 23, loss = 0.54198783\n",
            "Iteration 24, loss = 0.53988049\n",
            "Iteration 25, loss = 0.53787476\n",
            "Iteration 26, loss = 0.53576784\n",
            "Iteration 27, loss = 0.53382237\n",
            "Iteration 28, loss = 0.53192627\n",
            "Iteration 29, loss = 0.53005371\n",
            "Iteration 30, loss = 0.52819974\n",
            "Iteration 31, loss = 0.52641734\n",
            "Iteration 32, loss = 0.52471394\n",
            "Iteration 33, loss = 0.52301700\n",
            "Iteration 34, loss = 0.52137272\n",
            "Iteration 35, loss = 0.51968403\n",
            "Iteration 36, loss = 0.51803114\n",
            "Iteration 37, loss = 0.51619366\n",
            "Iteration 38, loss = 0.51432521\n",
            "Iteration 39, loss = 0.51247838\n",
            "Iteration 40, loss = 0.51068101\n",
            "Iteration 41, loss = 0.50886648\n",
            "Iteration 42, loss = 0.50702944\n",
            "Iteration 43, loss = 0.50515298\n",
            "Iteration 44, loss = 0.50323434\n",
            "Iteration 45, loss = 0.50135207\n",
            "Iteration 46, loss = 0.49944544\n",
            "Iteration 47, loss = 0.49758026\n",
            "Iteration 48, loss = 0.49578298\n",
            "Iteration 49, loss = 0.49410821\n",
            "Iteration 50, loss = 0.49234768\n",
            "Iteration 51, loss = 0.49060379\n",
            "Iteration 52, loss = 0.48905864\n",
            "Iteration 53, loss = 0.48752429\n",
            "Iteration 54, loss = 0.48593326\n",
            "Iteration 55, loss = 0.48449339\n",
            "Iteration 56, loss = 0.48310464\n",
            "Iteration 57, loss = 0.48174483\n",
            "Iteration 58, loss = 0.48042548\n",
            "Iteration 59, loss = 0.47909098\n",
            "Iteration 60, loss = 0.47783949\n",
            "Iteration 61, loss = 0.47655529\n",
            "Iteration 62, loss = 0.47528967\n",
            "Iteration 63, loss = 0.47402446\n",
            "Iteration 64, loss = 0.47290793\n",
            "Iteration 65, loss = 0.47178080\n",
            "Iteration 66, loss = 0.47060701\n",
            "Iteration 67, loss = 0.46946816\n",
            "Iteration 68, loss = 0.46836787\n",
            "Iteration 69, loss = 0.46734543\n",
            "Iteration 70, loss = 0.46633057\n",
            "Iteration 71, loss = 0.46529329\n",
            "Iteration 72, loss = 0.46424190\n",
            "Iteration 73, loss = 0.46316592\n",
            "Iteration 74, loss = 0.46212673\n",
            "Iteration 75, loss = 0.46106694\n",
            "Iteration 76, loss = 0.46004478\n",
            "Iteration 77, loss = 0.45897370\n",
            "Iteration 78, loss = 0.45791784\n",
            "Iteration 79, loss = 0.45692971\n",
            "Iteration 80, loss = 0.45594844\n",
            "Iteration 81, loss = 0.45499270\n",
            "Iteration 82, loss = 0.45405508\n",
            "Iteration 83, loss = 0.45313285\n",
            "Iteration 84, loss = 0.45222545\n",
            "Iteration 85, loss = 0.45130389\n",
            "Iteration 86, loss = 0.45037122\n",
            "Iteration 87, loss = 0.44943736\n",
            "Iteration 88, loss = 0.44850951\n",
            "Iteration 89, loss = 0.44760269\n",
            "Iteration 90, loss = 0.44672974\n",
            "Iteration 91, loss = 0.44588519\n",
            "Iteration 92, loss = 0.44507367\n",
            "Iteration 93, loss = 0.44428608\n",
            "Iteration 94, loss = 0.44350019\n",
            "Iteration 95, loss = 0.44271954\n",
            "Iteration 96, loss = 0.44191150\n",
            "Iteration 97, loss = 0.44112674\n",
            "Iteration 98, loss = 0.44031498\n",
            "Iteration 99, loss = 0.43952036\n",
            "Iteration 100, loss = 0.43873322\n",
            "Iteration 1, loss = 0.64331193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss = 0.62147704\n",
            "Iteration 3, loss = 0.60628433\n",
            "Iteration 4, loss = 0.59254682\n",
            "Iteration 5, loss = 0.57649001\n",
            "Iteration 6, loss = 0.55952787\n",
            "Iteration 7, loss = 0.54342182\n",
            "Iteration 8, loss = 0.52970531\n",
            "Iteration 9, loss = 0.51810136\n",
            "Iteration 10, loss = 0.50885326\n",
            "Iteration 11, loss = 0.50154355\n",
            "Iteration 12, loss = 0.49593569\n",
            "Iteration 13, loss = 0.49131298\n",
            "Iteration 14, loss = 0.48769939\n",
            "Iteration 15, loss = 0.48458671\n",
            "Iteration 16, loss = 0.48184417\n",
            "Iteration 17, loss = 0.47937805\n",
            "Iteration 18, loss = 0.47713299\n",
            "Iteration 19, loss = 0.47509969\n",
            "Iteration 20, loss = 0.47317576\n",
            "Iteration 21, loss = 0.47137084\n",
            "Iteration 22, loss = 0.46967443\n",
            "Iteration 23, loss = 0.46798131\n",
            "Iteration 24, loss = 0.46642500\n",
            "Iteration 25, loss = 0.46487310\n",
            "Iteration 26, loss = 0.46343842\n",
            "Iteration 27, loss = 0.46207242\n",
            "Iteration 28, loss = 0.46074348\n",
            "Iteration 29, loss = 0.45934575\n",
            "Iteration 30, loss = 0.45784450\n",
            "Iteration 31, loss = 0.45625718\n",
            "Iteration 32, loss = 0.45480477\n",
            "Iteration 33, loss = 0.45338241\n",
            "Iteration 34, loss = 0.45202297\n",
            "Iteration 35, loss = 0.45060502\n",
            "Iteration 36, loss = 0.44919532\n",
            "Iteration 37, loss = 0.44788676\n",
            "Iteration 38, loss = 0.44657557\n",
            "Iteration 39, loss = 0.44535039\n",
            "Iteration 40, loss = 0.44397191\n",
            "Iteration 41, loss = 0.44259520\n",
            "Iteration 42, loss = 0.44130892\n",
            "Iteration 43, loss = 0.43991970\n",
            "Iteration 44, loss = 0.43864306\n",
            "Iteration 45, loss = 0.43729208\n",
            "Iteration 46, loss = 0.43595647\n",
            "Iteration 47, loss = 0.43449599\n",
            "Iteration 48, loss = 0.43309143\n",
            "Iteration 49, loss = 0.43168978\n",
            "Iteration 50, loss = 0.43120123\n",
            "Iteration 51, loss = 0.43035814\n",
            "Iteration 52, loss = 0.42772336\n",
            "Iteration 53, loss = 0.42635407\n",
            "Iteration 54, loss = 0.42534323\n",
            "Iteration 55, loss = 0.42428132\n",
            "Iteration 56, loss = 0.42301342\n",
            "Iteration 57, loss = 0.42179487\n",
            "Iteration 58, loss = 0.42085020\n",
            "Iteration 59, loss = 0.42014831\n",
            "Iteration 60, loss = 0.41916615\n",
            "Iteration 61, loss = 0.41803856\n",
            "Iteration 62, loss = 0.41700900\n",
            "Iteration 63, loss = 0.41609865\n",
            "Iteration 64, loss = 0.41525288\n",
            "Iteration 65, loss = 0.41440391\n",
            "Iteration 66, loss = 0.41357203\n",
            "Iteration 67, loss = 0.41272121\n",
            "Iteration 68, loss = 0.41188240\n",
            "Iteration 69, loss = 0.41109548\n",
            "Iteration 70, loss = 0.41033162\n",
            "Iteration 71, loss = 0.40952874\n",
            "Iteration 72, loss = 0.40873338\n",
            "Iteration 73, loss = 0.40796531\n",
            "Iteration 74, loss = 0.40715052\n",
            "Iteration 75, loss = 0.40638453\n",
            "Iteration 76, loss = 0.40562770\n",
            "Iteration 77, loss = 0.40486616\n",
            "Iteration 78, loss = 0.40412366\n",
            "Iteration 79, loss = 0.40339710\n",
            "Iteration 80, loss = 0.40266856\n",
            "Iteration 81, loss = 0.40194765\n",
            "Iteration 82, loss = 0.40122732\n",
            "Iteration 83, loss = 0.40050966\n",
            "Iteration 84, loss = 0.39979211\n",
            "Iteration 85, loss = 0.39909478\n",
            "Iteration 86, loss = 0.39837232\n",
            "Iteration 87, loss = 0.39766890\n",
            "Iteration 88, loss = 0.39696828\n",
            "Iteration 89, loss = 0.39628441\n",
            "Iteration 90, loss = 0.39557588\n",
            "Iteration 91, loss = 0.39489074\n",
            "Iteration 92, loss = 0.39420561\n",
            "Iteration 93, loss = 0.39352265\n",
            "Iteration 94, loss = 0.39284755\n",
            "Iteration 95, loss = 0.39217511\n",
            "Iteration 96, loss = 0.39150794\n",
            "Iteration 97, loss = 0.39083693\n",
            "Iteration 98, loss = 0.39017196\n",
            "Iteration 99, loss = 0.38951110\n",
            "Iteration 100, loss = 0.38885216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.66048557\n",
            "Iteration 2, loss = 0.64042892\n",
            "Iteration 3, loss = 0.62728728\n",
            "Iteration 4, loss = 0.61509390\n",
            "Iteration 5, loss = 0.60059290\n",
            "Iteration 6, loss = 0.58379904\n",
            "Iteration 7, loss = 0.56834610\n",
            "Iteration 8, loss = 0.55536995\n",
            "Iteration 9, loss = 0.54472927\n",
            "Iteration 10, loss = 0.53653073\n",
            "Iteration 11, loss = 0.53035665\n",
            "Iteration 12, loss = 0.52551034\n",
            "Iteration 13, loss = 0.52172460\n",
            "Iteration 14, loss = 0.51864971\n",
            "Iteration 15, loss = 0.51607125\n",
            "Iteration 16, loss = 0.51378724\n",
            "Iteration 17, loss = 0.51174853\n",
            "Iteration 18, loss = 0.50991153\n",
            "Iteration 19, loss = 0.50808935\n",
            "Iteration 20, loss = 0.50640519\n",
            "Iteration 21, loss = 0.50475575\n",
            "Iteration 22, loss = 0.50316848\n",
            "Iteration 23, loss = 0.50163638\n",
            "Iteration 24, loss = 0.50012680\n",
            "Iteration 25, loss = 0.49868968\n",
            "Iteration 26, loss = 0.49726256\n",
            "Iteration 27, loss = 0.49588518\n",
            "Iteration 28, loss = 0.49453076\n",
            "Iteration 29, loss = 0.49320740\n",
            "Iteration 30, loss = 0.49186832\n",
            "Iteration 31, loss = 0.49053287\n",
            "Iteration 32, loss = 0.48918514\n",
            "Iteration 33, loss = 0.48781977\n",
            "Iteration 34, loss = 0.48651066\n",
            "Iteration 35, loss = 0.48518209\n",
            "Iteration 36, loss = 0.48390105\n",
            "Iteration 37, loss = 0.48264877\n",
            "Iteration 38, loss = 0.48139516\n",
            "Iteration 39, loss = 0.48015624\n",
            "Iteration 40, loss = 0.47894179\n",
            "Iteration 41, loss = 0.47771662\n",
            "Iteration 42, loss = 0.47645604\n",
            "Iteration 43, loss = 0.47518193\n",
            "Iteration 44, loss = 0.47393079\n",
            "Iteration 45, loss = 0.47236003\n",
            "Iteration 46, loss = 0.47103223\n",
            "Iteration 47, loss = 0.46963187\n",
            "Iteration 48, loss = 0.46832180\n",
            "Iteration 49, loss = 0.46703701\n",
            "Iteration 50, loss = 0.46572786\n",
            "Iteration 51, loss = 0.46439815\n",
            "Iteration 52, loss = 0.46308190\n",
            "Iteration 53, loss = 0.46172329\n",
            "Iteration 54, loss = 0.46038727\n",
            "Iteration 55, loss = 0.45900651\n",
            "Iteration 56, loss = 0.45749238\n",
            "Iteration 57, loss = 0.45636617\n",
            "Iteration 58, loss = 0.45516931\n",
            "Iteration 59, loss = 0.45329491\n",
            "Iteration 60, loss = 0.45155346\n",
            "Iteration 61, loss = 0.45020440\n",
            "Iteration 62, loss = 0.44898918\n",
            "Iteration 63, loss = 0.44773709\n",
            "Iteration 64, loss = 0.44638975\n",
            "Iteration 65, loss = 0.44501580\n",
            "Iteration 66, loss = 0.44346228\n",
            "Iteration 67, loss = 0.44207270\n",
            "Iteration 68, loss = 0.44072963\n",
            "Iteration 69, loss = 0.43940911\n",
            "Iteration 70, loss = 0.43814076\n",
            "Iteration 71, loss = 0.43691173\n",
            "Iteration 72, loss = 0.43568943\n",
            "Iteration 73, loss = 0.43447182\n",
            "Iteration 74, loss = 0.43329581\n",
            "Iteration 75, loss = 0.43216096\n",
            "Iteration 76, loss = 0.43105973\n",
            "Iteration 77, loss = 0.42998005\n",
            "Iteration 78, loss = 0.42890184\n",
            "Iteration 79, loss = 0.42784612\n",
            "Iteration 80, loss = 0.42680524\n",
            "Iteration 81, loss = 0.42578654\n",
            "Iteration 82, loss = 0.42476078\n",
            "Iteration 83, loss = 0.42374878\n",
            "Iteration 84, loss = 0.42274528\n",
            "Iteration 85, loss = 0.42175061\n",
            "Iteration 86, loss = 0.42076668\n",
            "Iteration 87, loss = 0.41982084\n",
            "Iteration 88, loss = 0.41907029\n",
            "Iteration 89, loss = 0.41813206\n",
            "Iteration 90, loss = 0.41703933\n",
            "Iteration 91, loss = 0.41597401\n",
            "Iteration 92, loss = 0.41497205\n",
            "Iteration 93, loss = 0.41402414\n",
            "Iteration 94, loss = 0.41307841\n",
            "Iteration 95, loss = 0.41214224\n",
            "Iteration 96, loss = 0.41121004\n",
            "Iteration 97, loss = 0.41027767\n",
            "Iteration 98, loss = 0.40934507\n",
            "Iteration 99, loss = 0.40842338\n",
            "Iteration 100, loss = 0.40749980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.73466865\n",
            "Iteration 2, loss = 0.69442233\n",
            "Iteration 3, loss = 0.67271794\n",
            "Iteration 4, loss = 0.66124813\n",
            "Iteration 5, loss = 0.65417115\n",
            "Iteration 6, loss = 0.64794049\n",
            "Iteration 7, loss = 0.64093249\n",
            "Iteration 8, loss = 0.63175404\n",
            "Iteration 9, loss = 0.61941212\n",
            "Iteration 10, loss = 0.60437519\n",
            "Iteration 11, loss = 0.59023847\n",
            "Iteration 12, loss = 0.57855363\n",
            "Iteration 13, loss = 0.56891199\n",
            "Iteration 14, loss = 0.56035937\n",
            "Iteration 15, loss = 0.55286268\n",
            "Iteration 16, loss = 0.54652101\n",
            "Iteration 17, loss = 0.54119042\n",
            "Iteration 18, loss = 0.53661190\n",
            "Iteration 19, loss = 0.53263004\n",
            "Iteration 20, loss = 0.52915154\n",
            "Iteration 21, loss = 0.52598401\n",
            "Iteration 22, loss = 0.52295875\n",
            "Iteration 23, loss = 0.52011620\n",
            "Iteration 24, loss = 0.51735826\n",
            "Iteration 25, loss = 0.51482856\n",
            "Iteration 26, loss = 0.51227592\n",
            "Iteration 27, loss = 0.50973652\n",
            "Iteration 28, loss = 0.50740975\n",
            "Iteration 29, loss = 0.50484619\n",
            "Iteration 30, loss = 0.50250312\n",
            "Iteration 31, loss = 0.50020034\n",
            "Iteration 32, loss = 0.49792403\n",
            "Iteration 33, loss = 0.49569423\n",
            "Iteration 34, loss = 0.49356355\n",
            "Iteration 35, loss = 0.49148215\n",
            "Iteration 36, loss = 0.48955045\n",
            "Iteration 37, loss = 0.48759312\n",
            "Iteration 38, loss = 0.48560478\n",
            "Iteration 39, loss = 0.48363473\n",
            "Iteration 40, loss = 0.48170451\n",
            "Iteration 41, loss = 0.47973806\n",
            "Iteration 42, loss = 0.47774141\n",
            "Iteration 43, loss = 0.47581280\n",
            "Iteration 44, loss = 0.47386168\n",
            "Iteration 45, loss = 0.47183775\n",
            "Iteration 46, loss = 0.46972715\n",
            "Iteration 47, loss = 0.46769714\n",
            "Iteration 48, loss = 0.46574473\n",
            "Iteration 49, loss = 0.46375854\n",
            "Iteration 50, loss = 0.46260750\n",
            "Iteration 51, loss = 0.46151383\n",
            "Iteration 52, loss = 0.45884588\n",
            "Iteration 53, loss = 0.45637982\n",
            "Iteration 54, loss = 0.45441029\n",
            "Iteration 55, loss = 0.45263776\n",
            "Iteration 56, loss = 0.45087268\n",
            "Iteration 57, loss = 0.44912581\n",
            "Iteration 58, loss = 0.44730976\n",
            "Iteration 59, loss = 0.44553602\n",
            "Iteration 60, loss = 0.44377998\n",
            "Iteration 61, loss = 0.44213360\n",
            "Iteration 62, loss = 0.44043988\n",
            "Iteration 63, loss = 0.43868446\n",
            "Iteration 64, loss = 0.43708535\n",
            "Iteration 65, loss = 0.43549543\n",
            "Iteration 66, loss = 0.43398768\n",
            "Iteration 67, loss = 0.43241501\n",
            "Iteration 68, loss = 0.43073192\n",
            "Iteration 69, loss = 0.42925025\n",
            "Iteration 70, loss = 0.42758235\n",
            "Iteration 71, loss = 0.42598468\n",
            "Iteration 72, loss = 0.42446210\n",
            "Iteration 73, loss = 0.42296386\n",
            "Iteration 74, loss = 0.42148520\n",
            "Iteration 75, loss = 0.42001837\n",
            "Iteration 76, loss = 0.41851577\n",
            "Iteration 77, loss = 0.41715479\n",
            "Iteration 78, loss = 0.41584089\n",
            "Iteration 79, loss = 0.41462011\n",
            "Iteration 80, loss = 0.41343507\n",
            "Iteration 81, loss = 0.41229605\n",
            "Iteration 82, loss = 0.41112498\n",
            "Iteration 83, loss = 0.40997091\n",
            "Iteration 84, loss = 0.40884115\n",
            "Iteration 85, loss = 0.40769623\n",
            "Iteration 86, loss = 0.40660215\n",
            "Iteration 87, loss = 0.40547969\n",
            "Iteration 88, loss = 0.40446146\n",
            "Iteration 89, loss = 0.40336701\n",
            "Iteration 90, loss = 0.40220537\n",
            "Iteration 91, loss = 0.40106962\n",
            "Iteration 92, loss = 0.40002547\n",
            "Iteration 93, loss = 0.39892992\n",
            "Iteration 94, loss = 0.39802761\n",
            "Iteration 95, loss = 0.39709821\n",
            "Iteration 96, loss = 0.39603948\n",
            "Iteration 97, loss = 0.39496325\n",
            "Iteration 98, loss = 0.39393303\n",
            "Iteration 99, loss = 0.39298606\n",
            "Iteration 100, loss = 0.39208622\n",
            "Iteration 1, loss = 0.62114753\n",
            "Iteration 2, loss = 0.61581237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3, loss = 0.61043700\n",
            "Iteration 4, loss = 0.60547516\n",
            "Iteration 5, loss = 0.60007680\n",
            "Iteration 6, loss = 0.59463461\n",
            "Iteration 7, loss = 0.58964300\n",
            "Iteration 8, loss = 0.58527984\n",
            "Iteration 9, loss = 0.58155342\n",
            "Iteration 10, loss = 0.57857036\n",
            "Iteration 11, loss = 0.57609163\n",
            "Iteration 12, loss = 0.57386760\n",
            "Iteration 13, loss = 0.57186398\n",
            "Iteration 14, loss = 0.57010512\n",
            "Iteration 15, loss = 0.56842151\n",
            "Iteration 16, loss = 0.56676948\n",
            "Iteration 17, loss = 0.56520489\n",
            "Iteration 18, loss = 0.56365817\n",
            "Iteration 19, loss = 0.56220989\n",
            "Iteration 20, loss = 0.56077495\n",
            "Iteration 21, loss = 0.55937695\n",
            "Iteration 22, loss = 0.55803879\n",
            "Iteration 23, loss = 0.55670929\n",
            "Iteration 24, loss = 0.55543634\n",
            "Iteration 25, loss = 0.55417428\n",
            "Iteration 26, loss = 0.55296740\n",
            "Iteration 27, loss = 0.55181515\n",
            "Iteration 28, loss = 0.55066091\n",
            "Iteration 29, loss = 0.54949219\n",
            "Iteration 30, loss = 0.54823428\n",
            "Iteration 31, loss = 0.54700803\n",
            "Iteration 32, loss = 0.54571721\n",
            "Iteration 33, loss = 0.54442385\n",
            "Iteration 34, loss = 0.54323350\n",
            "Iteration 35, loss = 0.54199051\n",
            "Iteration 36, loss = 0.54068784\n",
            "Iteration 37, loss = 0.53939570\n",
            "Iteration 38, loss = 0.53810963\n",
            "Iteration 39, loss = 0.53672816\n",
            "Iteration 40, loss = 0.53545457\n",
            "Iteration 41, loss = 0.53417630\n",
            "Iteration 42, loss = 0.53285417\n",
            "Iteration 43, loss = 0.53153526\n",
            "Iteration 44, loss = 0.53031722\n",
            "Iteration 45, loss = 0.52946943\n",
            "Iteration 46, loss = 0.52800131\n",
            "Iteration 47, loss = 0.52668400\n",
            "Iteration 48, loss = 0.52570057\n",
            "Iteration 49, loss = 0.52466784\n",
            "Iteration 50, loss = 0.52356036\n",
            "Iteration 51, loss = 0.52250507\n",
            "Iteration 52, loss = 0.52149933\n",
            "Iteration 53, loss = 0.52049878\n",
            "Iteration 54, loss = 0.51952189\n",
            "Iteration 55, loss = 0.51854266\n",
            "Iteration 56, loss = 0.51757886\n",
            "Iteration 57, loss = 0.51662195\n",
            "Iteration 58, loss = 0.51566573\n",
            "Iteration 59, loss = 0.51470881\n",
            "Iteration 60, loss = 0.51375232\n",
            "Iteration 61, loss = 0.51281404\n",
            "Iteration 62, loss = 0.51185585\n",
            "Iteration 63, loss = 0.51093782\n",
            "Iteration 64, loss = 0.51006220\n",
            "Iteration 65, loss = 0.50917589\n",
            "Iteration 66, loss = 0.50823003\n",
            "Iteration 67, loss = 0.50730328\n",
            "Iteration 68, loss = 0.50639256\n",
            "Iteration 69, loss = 0.50549408\n",
            "Iteration 70, loss = 0.50461630\n",
            "Iteration 71, loss = 0.50373940\n",
            "Iteration 72, loss = 0.50287516\n",
            "Iteration 73, loss = 0.50201948\n",
            "Iteration 74, loss = 0.50116587\n",
            "Iteration 75, loss = 0.50031114\n",
            "Iteration 76, loss = 0.49946216\n",
            "Iteration 77, loss = 0.49861376\n",
            "Iteration 78, loss = 0.49777105\n",
            "Iteration 79, loss = 0.49693033\n",
            "Iteration 80, loss = 0.49608868\n",
            "Iteration 81, loss = 0.49525929\n",
            "Iteration 82, loss = 0.49443846\n",
            "Iteration 83, loss = 0.49360287\n",
            "Iteration 84, loss = 0.49276835\n",
            "Iteration 85, loss = 0.49194898\n",
            "Iteration 86, loss = 0.49112749\n",
            "Iteration 87, loss = 0.49031929\n",
            "Iteration 88, loss = 0.48949584\n",
            "Iteration 89, loss = 0.48868321\n",
            "Iteration 90, loss = 0.48787168\n",
            "Iteration 91, loss = 0.48706101\n",
            "Iteration 92, loss = 0.48623481\n",
            "Iteration 93, loss = 0.48542667\n",
            "Iteration 94, loss = 0.48461564\n",
            "Iteration 95, loss = 0.48382076\n",
            "Iteration 96, loss = 0.48300802\n",
            "Iteration 97, loss = 0.48221156\n",
            "Iteration 98, loss = 0.48141518\n",
            "Iteration 99, loss = 0.48061534\n",
            "Iteration 100, loss = 0.47981816\n",
            "Iteration 1, loss = 0.80314261\n",
            "Iteration 2, loss = 0.75233040\n",
            "Iteration 3, loss = 0.71949764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4, loss = 0.70127739\n",
            "Iteration 5, loss = 0.69300825\n",
            "Iteration 6, loss = 0.68816778\n",
            "Iteration 7, loss = 0.68478994\n",
            "Iteration 8, loss = 0.68178251\n",
            "Iteration 9, loss = 0.67873609\n",
            "Iteration 10, loss = 0.67515256\n",
            "Iteration 11, loss = 0.67067641\n",
            "Iteration 12, loss = 0.66462482\n",
            "Iteration 13, loss = 0.65585570\n",
            "Iteration 14, loss = 0.64592175\n",
            "Iteration 15, loss = 0.63527584\n",
            "Iteration 16, loss = 0.62630629\n",
            "Iteration 17, loss = 0.61900699\n",
            "Iteration 18, loss = 0.61276090\n",
            "Iteration 19, loss = 0.60731214\n",
            "Iteration 20, loss = 0.60234142\n",
            "Iteration 21, loss = 0.59793875\n",
            "Iteration 22, loss = 0.59408290\n",
            "Iteration 23, loss = 0.59077230\n",
            "Iteration 24, loss = 0.58788189\n",
            "Iteration 25, loss = 0.58520413\n",
            "Iteration 26, loss = 0.58275339\n",
            "Iteration 27, loss = 0.58045051\n",
            "Iteration 28, loss = 0.57824843\n",
            "Iteration 29, loss = 0.57606810\n",
            "Iteration 30, loss = 0.57397352\n",
            "Iteration 31, loss = 0.57191467\n",
            "Iteration 32, loss = 0.56991119\n",
            "Iteration 33, loss = 0.56793725\n",
            "Iteration 34, loss = 0.56601438\n",
            "Iteration 35, loss = 0.56405696\n",
            "Iteration 36, loss = 0.56220073\n",
            "Iteration 37, loss = 0.56031637\n",
            "Iteration 38, loss = 0.55847276\n",
            "Iteration 39, loss = 0.55669021\n",
            "Iteration 40, loss = 0.55499229\n",
            "Iteration 41, loss = 0.55325654\n",
            "Iteration 42, loss = 0.55156586\n",
            "Iteration 43, loss = 0.54990710\n",
            "Iteration 44, loss = 0.54824468\n",
            "Iteration 45, loss = 0.54663916\n",
            "Iteration 46, loss = 0.54502761\n",
            "Iteration 47, loss = 0.54344037\n",
            "Iteration 48, loss = 0.54186974\n",
            "Iteration 49, loss = 0.54024102\n",
            "Iteration 50, loss = 0.53865321\n",
            "Iteration 51, loss = 0.53704558\n",
            "Iteration 52, loss = 0.53546855\n",
            "Iteration 53, loss = 0.53392494\n",
            "Iteration 54, loss = 0.53237466\n",
            "Iteration 55, loss = 0.53081972\n",
            "Iteration 56, loss = 0.52923016\n",
            "Iteration 57, loss = 0.52761685\n",
            "Iteration 58, loss = 0.52591577\n",
            "Iteration 59, loss = 0.52419379\n",
            "Iteration 60, loss = 0.52244723\n",
            "Iteration 61, loss = 0.52078859\n",
            "Iteration 62, loss = 0.51909815\n",
            "Iteration 63, loss = 0.51750185\n",
            "Iteration 64, loss = 0.51592508\n",
            "Iteration 65, loss = 0.51430068\n",
            "Iteration 66, loss = 0.51273969\n",
            "Iteration 67, loss = 0.51106588\n",
            "Iteration 68, loss = 0.50945468\n",
            "Iteration 69, loss = 0.50783083\n",
            "Iteration 70, loss = 0.50623950\n",
            "Iteration 71, loss = 0.50448775\n",
            "Iteration 72, loss = 0.50281911\n",
            "Iteration 73, loss = 0.50112980\n",
            "Iteration 74, loss = 0.49946616\n",
            "Iteration 75, loss = 0.49780591\n",
            "Iteration 76, loss = 0.49604619\n",
            "Iteration 77, loss = 0.49431397\n",
            "Iteration 78, loss = 0.49243746\n",
            "Iteration 79, loss = 0.49066193\n",
            "Iteration 80, loss = 0.48898352\n",
            "Iteration 81, loss = 0.48728364\n",
            "Iteration 82, loss = 0.48566769\n",
            "Iteration 83, loss = 0.48413071\n",
            "Iteration 84, loss = 0.48285894\n",
            "Iteration 85, loss = 0.48141515\n",
            "Iteration 86, loss = 0.47976676\n",
            "Iteration 87, loss = 0.47814990\n",
            "Iteration 88, loss = 0.47675419\n",
            "Iteration 89, loss = 0.47540216\n",
            "Iteration 90, loss = 0.47403809\n",
            "Iteration 91, loss = 0.47264949\n",
            "Iteration 92, loss = 0.47127366\n",
            "Iteration 93, loss = 0.46992875\n",
            "Iteration 94, loss = 0.46859740\n",
            "Iteration 95, loss = 0.46728578\n",
            "Iteration 96, loss = 0.46598782\n",
            "Iteration 97, loss = 0.46467510\n",
            "Iteration 98, loss = 0.46339277\n",
            "Iteration 99, loss = 0.46211568\n",
            "Iteration 100, loss = 0.46082744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculons le mean de chaque validation croisee :"
      ],
      "metadata": {
        "id": "abN4UcxXjmI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(round(accuracy1.mean(),3))\n",
        "print(round(accuracy2.mean(),3))\n",
        "print(round(accuracy3.mean(),3))\n",
        "print(round(accuracy4.mean(),3))\n",
        "np.round(accuracy4,3)"
      ],
      "metadata": {
        "id": "qHHAWBBtPf7V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1047e03b-a9bf-4040-aebf-b35f6ef72c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.942\n",
            "1.0\n",
            "0.98\n",
            "0.816\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.689, 0.689, 0.689, 0.989, 0.7  , 0.7  , 1.   , 1.   , 0.7  ,\n",
              "       1.   ])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "rassamplons nos resultas dans un tableaux conient le modele et  le mean de la presion lors de l'application de la validation croisee "
      ],
      "metadata": {
        "id": "V8AfFjHVjx9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_comp = {\" Model \":[\"knn\",\"svm\",\"svm gaussian\",\"Multilayer perceptron\"],\"  10-fold Cross validation score  \":[np.round(accuracy1,3),np.round(accuracy2,3),np.round(accuracy3,3),np.round(accuracy4,3)],\n",
        "             \"  Average Score  \":[round(accuracy1.mean(),3),round(accuracy2.mean(),3),round(accuracy3.mean(),3),round(accuracy4.mean(),3)]}\n",
        "import pandas as pd\n",
        "model_comp= pd.DataFrame(model_comp)\n",
        "model_comp"
      ],
      "metadata": {
        "id": "rjiGjl41P5xD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "67578962-0d3a-4e24-c8d8-ad9e1ac5644e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  Model                    10-fold Cross validation score    \\\n",
              "0                    knn  [0.911, 0.911, 0.944, 0.889, 0.967, 0.967, 0.9...   \n",
              "1                    svm  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
              "2           svm gaussian  [0.978, 0.967, 0.967, 0.967, 0.989, 0.978, 0.9...   \n",
              "3  Multilayer perceptron  [0.689, 0.689, 0.689, 0.989, 0.7, 0.7, 1.0, 1....   \n",
              "\n",
              "     Average Score    \n",
              "0              0.942  \n",
              "1              1.000  \n",
              "2              0.980  \n",
              "3              0.816  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-799f2a2d-fa24-4ff3-900f-a3344acb79f8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>10-fold Cross validation score</th>\n",
              "      <th>Average Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>knn</td>\n",
              "      <td>[0.911, 0.911, 0.944, 0.889, 0.967, 0.967, 0.9...</td>\n",
              "      <td>0.942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>svm</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>svm gaussian</td>\n",
              "      <td>[0.978, 0.967, 0.967, 0.967, 0.989, 0.978, 0.9...</td>\n",
              "      <td>0.980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Multilayer perceptron</td>\n",
              "      <td>[0.689, 0.689, 0.689, 0.989, 0.7, 0.7, 1.0, 1....</td>\n",
              "      <td>0.816</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-799f2a2d-fa24-4ff3-900f-a3344acb79f8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-799f2a2d-fa24-4ff3-900f-a3344acb79f8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-799f2a2d-fa24-4ff3-900f-a3344acb79f8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "on fait un prediction sur les donnees que on a reserve pour le test "
      ],
      "metadata": {
        "id": "gpSHniHYkIsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ypredict_knn=knn.predict(X_test)\n",
        "ypredict_svm=svm.predict(X_test)\n",
        "ypredict_svmGaussian=svmGaussian.predict(X_test)\n",
        "ypredict_mlp=mlp_clf.predict(X_test)\n"
      ],
      "metadata": {
        "id": "UbjPj7nJTIZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "utilisons les resulats reeles et notre pridiction pour generer le matice de confusion de chaqu'un de nos modeles"
      ],
      "metadata": {
        "id": "qpM89DQPkgpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "sns.set(rc={'figure.figsize':(18,12)})\n",
        "cm = confusion_matrix(y_test, ypredict_knn)\n",
        "sns.heatmap(cm,annot=True,fmt='2.0f')"
      ],
      "metadata": {
        "id": "YS7HyKL4T0Xc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "outputId": "2d158278-5f68-4cbb-fcd8-add34f86cd6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0b3a64afd0>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x864 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "on calcule les indicateurs des métriques de modele KNN : \n",
        "- Accuracy =(TN+TP)/(TP+TN+FN+FP)=(63+32)/(63+32+5)=0.95\n",
        "- Precision =TP/(TP+FP)=1\n",
        "- Negative Predictive Value=TN/(TN+FN)=63/(63+5)=0.93\n",
        "- Specificity =TP/(TN+FP)=32/(63+0)=0.5\n",
        "- Sensitivity =TP/(TP+FN)=32/(32+5)=0.82"
      ],
      "metadata": {
        "id": "Xg3FhtA7rAWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm1 = confusion_matrix(y_test, ypredict_svm)\n",
        "sns.heatmap(cm1,annot=True,fmt='2.0f')\n"
      ],
      "metadata": {
        "id": "xdHqlxnxWgNj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "outputId": "2a57f6ed-4e31-44a2-fd62-daaa0ff12b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0b39971c70>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x864 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "on calcule les indicateurs des métriques de modele SVM lineair :\n",
        "\n",
        "- Accuracy =(TN+TP)/(TP+TN+FN+FP)=1\n",
        "- Precision =TP/(TP+FP)=1\n",
        "- Negative Predictive Value=TN/(TN+FN)=1\n",
        "- Specificity =TP/(TN+FP)=0.58\n",
        "- Sensitivity =TP/(TP+FN)=1"
      ],
      "metadata": {
        "id": "eqp_xLzsoB1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm2 = confusion_matrix(y_test, ypredict_svmGaussian)\n",
        "sns.heatmap(cm2,annot=True,fmt='2.0f')\n"
      ],
      "metadata": {
        "id": "4pvHur56ZJEN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "outputId": "5082b8ca-dd30-4aa6-f81b-05cd84194833"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0b39875be0>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x864 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oR-3sRAK4CJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "on calcule les indicateurs des métriques de modele SVM Gaussian :\n",
        "\n",
        "- Accuracy =(TN+TP)/(TP+TN+FN+FP)=1\n",
        "- Precision =TP/(TP+FP)=1\n",
        "- Negative Predictive Value=TN/(TN+FN)=1\n",
        "- Specificity =TP/(TN+FP)=0.58\n",
        "- Sensitivity =TP/(TP+FN)=1"
      ],
      "metadata": {
        "id": "dpVRY-gcoaDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm3 = confusion_matrix(y_test, ypredict_mlp)\n",
        "sns.heatmap(cm1,annot=True,fmt='2.0f')"
      ],
      "metadata": {
        "id": "69uT7ppl4Dwa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "outputId": "9d96b0e5-6f95-4c7c-9fcd-60518bca1898"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0b39782a30>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x864 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}